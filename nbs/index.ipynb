{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WhisperSpeech\n",
    "\n",
    "An Open Source text-to-speech system built by inverting Whisper. Previously known as **spear-tts-pytorch**.\n",
    "\n",
    "The general architecture is similar to [AudioLM](https://google-research.github.io/seanet/audiolm/examples/), [SPEAR TTS](https://google-research.github.io/seanet/speartts/examples/) from Google and [MusicGen](https://ai.honu.io/papers/musicgen/) from Meta but we avoided the NIH syndrome and built it on top of powerful Open Source models: [Whisper](https://github.com/openai/whisper) from OpenAI to generate semantic tokens and perform transcription, [EnCodec](https://github.com/facebookresearch/encodec) from Meta for acoustic modeling and [Vocos](https://github.com/charactr-platform/vocos) from Charactr Inc as the high-quality vocoder.\n",
    "\n",
    "Currently the models are trained on the English LibreLight and LibreTTS datasets. Ultimately we want to target multiple languages (Whisper and EnCodec are both multilanguage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress updates\n",
    "\n",
    "**UPDATE 2023-07-14**: We have trained a new pair of models, added support for multiple speakers and integrated the\n",
    "    Vocos vocoder to deliver a big overall quality boost. And this is not even our last word because we are doing\n",
    "    hyperparameter tuning to train bigger, higher-quality models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roadmap\n",
    "\n",
    "- [x] [Extract acoustic tokens](https://github.com/collabora/spear-tts-pytorch/issues/2)\n",
    "- [x] [Extract Whisper embeddings and quantize them to semantic tokens](https://github.com/collabora/spear-tts-pytorch/issues/3)\n",
    "- [x] [Semantic token to acoustic token (S->A) model](https://github.com/collabora/spear-tts-pytorch/issues/4)\n",
    "- [x] [Text token to semantic token (T->S) model](https://github.com/collabora/spear-tts-pytorch/issues/9)\n",
    "- [x] [Improve the EnCodec speech quality](https://github.com/collabora/spear-tts-pytorch/issues/10)\n",
    "- [ ] [Gather a bigger emotive speech dataset](https://github.com/collabora/spear-tts-pytorch/issues/11)\n",
    "- [ ] [Train final high-quality models](https://github.com/collabora/spear-tts-pytorch/issues/12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whisper for modeling semantic tokens\n",
    "\n",
    "We utilize the OpenAI Whisper encoder block to generate embeddings which we then quantize with a small 2-layer model to get semantic tokens.\n",
    "\n",
    "If the language is already supported by Whisper then this process requires only audio files (without ground truth transcriptions).\n",
    "\n",
    "![Using Whisper for semantic token extraction diagram](whisper-block.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EnCodec for modeling acoustic tokens\n",
    "\n",
    "We use EnCodec to model the audio waveform. Out of the box it delivers reasonable quality at 1.5kbps and we can bring this to high-quality by using Vocos – a vocoder pretrained on EnCodec tokens.\n",
    "\n",
    "![EnCodec block diagram](https://github.com/facebookresearch/encodec/raw/main/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appreciation\n",
    "\n",
    "[<img height=80 src=\"https://user-images.githubusercontent.com/107984/229537027-a6d7462b-0c9c-4fd4-b69e-58e98c3ee63f.png\" alt=\"Collabora logo\">](https://www.collabora.com)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[<img height=80 src=\"https://user-images.githubusercontent.com/107984/229535036-c741d775-4a9b-4193-89a0-9ddb89ecd011.png\" alt=\"LAION logo\">](https://laion.ai)\n",
    "\n",
    "This work would not be possible without the generous sponsorships from:\n",
    "\n",
    "- [Collabora](https://www.collabora.com) – code development and model training\n",
    "- [LAION](https://laion.ai) – community building and datasets\n",
    "\n",
    "We are available to help you with both Open Source and proprietary AI projects. You can reach us via the Collabora website or on Discord ([![](https://dcbadge.vercel.app/api/shield/270267134960074762?style=flat)](https://discordapp.com/users/270267134960074762) and [![](https://dcbadge.vercel.app/api/shield/1088938086400016475?style=flat)](https://discordapp.com/users/1088938086400016475))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "We rely on many amazing Open Source projects and research papers:\n",
    "\n",
    "```bibtex\n",
    "@article{SpearTTS,\n",
    "  title = {Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision},\n",
    "  url = {https://arxiv.org/abs/2302.03540},\n",
    "  author = {Kharitonov, Eugene and Vincent, Damien and Borsos, Zalán and Marinier, Raphaël and Girgin, Sertan and Pietquin, Olivier and Sharifi, Matt and Tagliasacchi, Marco and Zeghidour, Neil},\n",
    "  publisher = {arXiv},\n",
    "  year = {2023},\n",
    "}\n",
    "```\n",
    "\n",
    "```bibtex\n",
    "@article{MusicGen,\n",
    "  title={Simple and Controllable Music Generation}, \n",
    "  url = {https://arxiv.org/abs/2306.05284},\n",
    "  author={Jade Copet and Felix Kreuk and Itai Gat and Tal Remez and David Kant and Gabriel Synnaeve and Yossi Adi and Alexandre Défossez},\n",
    "  publisher={arXiv},\n",
    "  year={2023},\n",
    "}\n",
    "\n",
    "```bibtex\n",
    "@article{Whisper\n",
    "  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n",
    "  url = {https://arxiv.org/abs/2212.04356},\n",
    "  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n",
    "  publisher = {arXiv},\n",
    "  year = {2022},\n",
    "}\n",
    "```\n",
    "\n",
    "```bibtex\n",
    "@article{EnCodec\n",
    "  title = {High Fidelity Neural Audio Compression},\n",
    "  url = {https://arxiv.org/abs/2210.13438},\n",
    "  author = {Défossez, Alexandre and Copet, Jade and Synnaeve, Gabriel and Adi, Yossi},\n",
    "  publisher = {arXiv},\n",
    "  year = {2022},\n",
    "}\n",
    "```\n",
    "\n",
    "```bibtex\n",
    "@article{Vocos\n",
    "      title={Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis}, \n",
    "      url = {https://arxiv.org/abs/2306.00814},\n",
    "      author={Hubert Siuzdak},\n",
    "      publisher={arXiv},\n",
    "      year={2023},\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
